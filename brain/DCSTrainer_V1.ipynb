{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yS7DRtljkE5o"
   },
   "source": [
    "# Modèle V1.4 de voiture autonome sur DonkeyCarSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xVi2MlJ6kSt8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgfVDF0Vj1cu",
    "outputId": "493cfe5a-41db-4932-84f1-12da35e5c977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok.\n"
     ]
    }
   ],
   "source": [
    "TIME=str(time())\n",
    "\n",
    "MODEL_NAME = \"DCModelV1.4-renault-\"+TIME\n",
    "\n",
    "STORAGE_ROOT_DIR = \"/home/nigiva/git/lopilo-trainer/data\"\n",
    "\n",
    "DATASET_NAME = \"renault_merged_58800\"\n",
    "DATASET_LABEL_FILENAME = \"label.csv\"\n",
    "\n",
    "DATASET_PATH = os.path.join(STORAGE_ROOT_DIR, \"sample\", \"extern\", DATASET_NAME)\n",
    "DATASET_LABEL_PATH = os.path.join(DATASET_PATH, DATASET_LABEL_FILENAME)\n",
    "TENSORLOG_PATH = os.path.join(STORAGE_ROOT_DIR, \"log\", \"extern\", MODEL_NAME)\n",
    "os.mkdir(TENSORLOG_PATH)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "SAVE_PATH = os.path.join(STORAGE_ROOT_DIR, \"model\", \"extern\", MODEL_NAME)\n",
    "IMAGE_SHAPE = (120,160, 1)\n",
    "print(\"Ok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Extraire la dataset\n",
    "## A) zip en dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-c9nYNl_l-4b",
    "outputId": "a479ee34-dea1-4109-d07d-52dccc806a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_controller_generated_track_0_48000  3_controller_generated_track_0_48000.zip\n"
     ]
    }
   ],
   "source": [
    "!rm -Rf \"/home/nigiva/git/dkarstream/Samples/3_controller_generated_track_0_48000\"\n",
    "!unzip -q \"/home/nigiva/git/dkarstream/Samples/3_controller_generated_track_0_48000.zip\" -d \"/home/nigiva/git/dkarstream/Samples/\"\n",
    "!ls \"/home/nigiva/git/dkarstream/Samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) json (.eslr) en dataset\n",
    "On convertit ligne par ligne les données envoyées par le serveur en :\n",
    "- une image\n",
    "- une ligne dans le csv label.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf \"/home/nigiva/git/lopilo-trainer/data/sample/extern/renault_merged_58800\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_JSON_FILE = DATASET_PATH + \".eslr\"\n",
    "\n",
    "IMAGE_PATH = \"images\"\n",
    "DATASET_IMAGE_PATH = os.path.join(DATASET_PATH, IMAGE_PATH)\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    os.mkdir(DATASET_PATH)\n",
    "if not os.path.exists(DATASET_IMAGE_PATH):\n",
    "    os.mkdir(DATASET_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58821it [01:13, 804.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_LABEL_FILE = open(DATASET_LABEL_PATH, \"w\")\n",
    "DATA_LABEL_FILE.write(\"path,angle,throttle\\n\")\n",
    "with open(DATASET_JSON_FILE, \"r\") as dataset_file:\n",
    "    for i, line in enumerate(tqdm(dataset_file)):\n",
    "        data_line = json.loads(line)\n",
    "        if (data_line[\"msg_type\"] == \"telemetry\"):\n",
    "            image_relative_path = os.path.join(IMAGE_PATH, str(i)+\".jpg\")\n",
    "            image_absolute_path = os.path.join(DATASET_PATH, image_relative_path)\n",
    "            Image.open(BytesIO(base64.b64decode(data_line[\"image\"]))).save(image_absolute_path)\n",
    "            # Toutes les données à ajouter dans la dataset\n",
    "            data2write = [image_relative_path, str(data_line[\"steering_angle\"]), str(data_line[\"throttle\"])]\n",
    "            DATA_LABEL_FILE.write(\",\".join(data2write) + \"\\n\")\n",
    "DATA_LABEL_FILE.close()\n",
    "print(\"ok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9kc5ebCnPWV"
   },
   "source": [
    "## 1. Préparer la dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaHmD2Xdn3Ij",
    "outputId": "a5f3d4b2-f984-4d06-af7d-be58fbc6ced4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    path     angle  throttle\n",
      "0      /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.098022       0.2\n",
      "1      /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.239227       0.2\n",
      "2      /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.239227       0.2\n",
      "3      /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.239227       0.2\n",
      "4      /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.364716       0.2\n",
      "...                                                  ...       ...       ...\n",
      "58815  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.152924       0.2\n",
      "58816  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.160767       0.2\n",
      "58817  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.160767       0.2\n",
      "58818  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.160767       0.2\n",
      "58819  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.160767       0.2\n",
      "\n",
      "[58820 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Obetnir les valeurs du fichier contenant les labels\n",
    "raw_data = pd.read_csv(DATASET_LABEL_PATH)\n",
    "\n",
    "# change la chemin de fichier\n",
    "def change_path(path):\n",
    "  return os.path.join(DATASET_PATH, path)\n",
    "raw_data['path'] = raw_data['path'].map(change_path)\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebOWYPc0pm9u"
   },
   "source": [
    "### Split en 3 jeux : Train, Test et Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHUJcUZGpunH",
    "outputId": "8398c538-0be3-48ac-f173-1b07b9060372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    path     angle  throttle\n",
      "31557  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.160767       0.2\n",
      "42010  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "3662   /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.239227       0.2\n",
      "5458   /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "2493   /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.098053       0.2\n",
      "...                                                  ...       ...       ...\n",
      "43508  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "38484  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.419586       0.2\n",
      "18203  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.129394       0.2\n",
      "52707  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.505859       0.2\n",
      "34231  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "\n",
      "[39997 rows x 3 columns]\n",
      "                                                    path     angle  throttle\n",
      "12843  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.278412       0.2\n",
      "52641  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.207825       0.2\n",
      "7717   /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.435303       0.2\n",
      "14729  /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.168640       0.2\n",
      "963    /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.231354       0.2\n",
      "...                                                  ...       ...       ...\n",
      "28393  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "50295  /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.419617       0.2\n",
      "40266  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.640000       0.2\n",
      "38661  /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.105896       0.2\n",
      "13863  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "                                                    path     angle  throttle\n",
      "12082  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "21569  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.521545       0.2\n",
      "47407  /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.349029       0.2\n",
      "58050  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "10511  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "...                                                  ...       ...       ...\n",
      "52672  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.640000       0.2\n",
      "46553  /home/nigiva/git/lopilo-trainer/data/sample/ex... -0.640000       0.2\n",
      "22396  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.545075       0.2\n",
      "14371  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.000000       0.2\n",
      "44362  /home/nigiva/git/lopilo-trainer/data/sample/ex...  0.199982       0.2\n",
      "\n",
      "[8823 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "train_and_test_set, validation_set = train_test_split(raw_data,\n",
    "                                             test_size = 0.15,\n",
    "                                             shuffle = True)\n",
    "train_set, test_set = train_test_split(train_and_test_set,\n",
    "                                             test_size = 0.20,\n",
    "                                             shuffle = True)\n",
    "NBR_ROW_TRAIN_SET = train_set.shape[0]\n",
    "NBR_ROW_TEST_SET = test_set.shape[0]\n",
    "NBR_ROW_VALIDATION_SET = validation_set.shape[0]\n",
    "print(train_set)\n",
    "print(test_set)\n",
    "print(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyHa_tlu2u01"
   },
   "source": [
    "### Traitements avec TensorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpP4VpM_s4tk",
    "outputId": "c7733e72-460b-4f7f-8e3d-21c0a4a34c31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(120, 160, 1), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(120, 160, 1), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(120, 160, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Mettre dans des tensors\n",
    "train_tensor = tf.data.Dataset.from_tensor_slices(({\"input\" : train_set['path']}, {\"angle\" : train_set['angle'], \"throttle\" : train_set['throttle']}))\n",
    "test_tensor = tf.data.Dataset.from_tensor_slices(({\"input\" : test_set['path']}, {\"angle\" : test_set['angle'], \"throttle\" : test_set['throttle']}))\n",
    "validation_tensor = tf.data.Dataset.from_tensor_slices(({\"input\" : validation_set['path']}, {\"angle\" : validation_set['angle'], \"throttle\" : validation_set['throttle']}))\n",
    "\n",
    "# Definir les fonctions de chargement des images et de mapping\n",
    "def load_and_preprocess_image(path):\n",
    "    file_content = tf.io.read_file(path['input'])\n",
    "    tricolors_img = tf.cast(tf.image.decode_jpeg(file_content, channels=3), dtype=tf.float32)\n",
    "    gray_img = tf.image.rgb_to_grayscale(tricolors_img)\n",
    "    # FIX change / 255.0 by / 255.0  -  0.5, to have a range between -0.5 et 0.5 (instead of 0 and 1)\n",
    "    normalized_img = (gray_img / 127.5) - 1\n",
    "    normalized_img = tf.reshape(normalized_img, IMAGE_SHAPE)\n",
    "    print(normalized_img)\n",
    "    return {\"input\": normalized_img}\n",
    "\n",
    "def load_map_function(path, d):\n",
    "    return load_and_preprocess_image(path), d\n",
    "\n",
    "# Appliquer le mapping aux tensors\n",
    "train_tensor_normalized = train_tensor.map(load_map_function, num_parallel_calls=3)\n",
    "test_tensor_normalized = test_tensor.map(load_map_function, num_parallel_calls=3)\n",
    "validation_tensor_normalized = validation_tensor.map(load_map_function, num_parallel_calls=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bUTvPnPazWT1"
   },
   "outputs": [],
   "source": [
    "# On mélange les datasets (on fixe un nombre d'exemple tiré au sort, ici 20 000 exemples sur les 30 000)\n",
    "# On demande un prechargement à l'avance de toujours 3 exemples\n",
    "train_dataset = train_tensor_normalized.shuffle(20000).batch(BATCH_SIZE).prefetch(2)\n",
    "test_dataset = test_tensor_normalized.shuffle(8000).batch(BATCH_SIZE).prefetch(2)\n",
    "validation_dataset = validation_tensor_normalized.shuffle(8000).batch(BATCH_SIZE).prefetch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9IyqtYc22X8"
   },
   "source": [
    "## 2. Le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSaver:\n",
    "  \"\"\"\n",
    "  Save the model into a file ('model.code') as source code\n",
    "  \"\"\"\n",
    "  def __init__(self, activate = True):\n",
    "    \"\"\"\n",
    "    Initialize the saver\n",
    "    :param activate: capture or not the source code of decorated functions\n",
    "    \"\"\"\n",
    "    self.is_activated = activate\n",
    "    self.s_init = None\n",
    "    self.s_call = None\n",
    "  def init(self, funct):\n",
    "    \"\"\"\n",
    "    Capture the source code of the init function\n",
    "    ---\n",
    "    Use as a decoration\n",
    "    Such as :\n",
    "    ```\n",
    "    MODEL_SAVER = ModelSaver(True)\n",
    "    ...\n",
    "    @MODEL_SAVER.init\n",
    "    def __init__(self, name = \"\"):\n",
    "      ...\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def funct_with_params(*args, **kwargs):\n",
    "      return funct(*args, **kwargs)\n",
    "    if self.is_activated:\n",
    "      self.s_init = inspect.getsource(funct)\n",
    "    return funct_with_params\n",
    "\n",
    "  def call(self, funct):\n",
    "    \"\"\"\n",
    "    Capture the source code of the init function\n",
    "    ---\n",
    "    Use as a decoration\n",
    "    Such as :\n",
    "    ```\n",
    "        MODEL_SAVER = ModelSaver(True)\n",
    "        ...\n",
    "        @MODEL_SAVER.call\n",
    "        def call(self):\n",
    "            ...\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def funct_with_params(*args, **kwargs):\n",
    "      return funct(*args, **kwargs)\n",
    "    if self.is_activated:\n",
    "      self.s_call = inspect.getsource(funct)\n",
    "    return funct_with_params\n",
    "  \n",
    "  def save(self, path):\n",
    "    \"\"\"\n",
    "    Save the source code of the model as a file\n",
    "    :param path: file path\n",
    "    \"\"\"\n",
    "    if self.s_init is not None and self.s_call is not None:\n",
    "      with open(path, \"w\") as s:\n",
    "        s.write(\"class DCModel(keras.Model):\\n\")\n",
    "        s.write(\"  MODEL_SAVER = ModelSaver(False)\\n\")\n",
    "        s.write(self.s_init)\n",
    "        s.write(self.s_call)\n",
    "    else:\n",
    "      raise Exception(\"init or call function are not saved\")\n",
    "  @staticmethod\n",
    "  def load(path):\n",
    "    \"\"\"\n",
    "    Load the Model source code\n",
    "    :param path: file path\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as s:\n",
    "      exec(s.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "cbJn9FmbOap1"
   },
   "outputs": [],
   "source": [
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "DpantkTMDLWc"
   },
   "outputs": [],
   "source": [
    "MODEL_SAVER = ModelSaver(True)\n",
    "\n",
    "class DCModel(keras.Model):\n",
    "  @MODEL_SAVER.init\n",
    "  def __init__(self, name=\"DCModel\"):\n",
    "    super(DCModel, self).__init__(name=name)\n",
    "    #self.input_layer = keras.layers.Input(shape=IMAGE_SHAPE, name='input')\n",
    "    self.cnn_1 = keras.layers.Conv2D(12, (5, 5), strides=(2, 2), padding=\"same\", kernel_initializer='he_uniform', activation='relu', name='input')\n",
    "    self.bn_1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.cnn_2 = keras.layers.Conv2D(16, (5, 5), strides=(2, 2), padding=\"same\", kernel_initializer='he_uniform', activation='relu')\n",
    "    self.bn_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.cnn_3 = keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer='he_uniform', activation='relu')\n",
    "    self.bn_3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.cnn_4 = keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer='he_uniform', activation='relu')\n",
    "    self.bn_4 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.cnn_5 = keras.layers.Conv2D(48, (3, 3), strides=(2, 2), padding=\"same\", kernel_initializer='he_uniform', activation='relu')\n",
    "    self.bn_5 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.drop_1 = keras.layers.Dropout(0.2)\n",
    "\n",
    "    self.flat = keras.layers.Flatten()\n",
    "\n",
    "    self.dense_1 = keras.layers.Dense(100, kernel_initializer='he_uniform', activation='elu')\n",
    "    self.bn_6 = tf.keras.layers.BatchNormalization()\n",
    "    self.drop_2 = keras.layers.Dropout(0.1)\n",
    "\n",
    "    self.dense_2 = keras.layers.Dense(50, kernel_initializer='he_uniform', activation='elu')\n",
    "    self.bn_7 = tf.keras.layers.BatchNormalization()\n",
    "    self.drop_3 = keras.layers.Dropout(0.1)\n",
    "\n",
    "    self.dense_3 = keras.layers.Dense(25, kernel_initializer='he_uniform', activation='elu')\n",
    "    self.bn_8 = tf.keras.layers.BatchNormalization()\n",
    "    self.drop_4 = keras.layers.Dropout(0.1)\n",
    "\n",
    "    self.dense_4 = keras.layers.Dense(25, kernel_initializer='he_uniform', activation='elu')\n",
    "    self.bn_9 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.output_layer_1 = keras.layers.Dense(1, activation='linear', name='angle')\n",
    "    self.output_layer_2 = keras.layers.Dense(1, activation='linear', name='throttle')\n",
    "\n",
    "  \n",
    "  @MODEL_SAVER.call\n",
    "  @tf.function\n",
    "  def call(self, inputs, training=False):\n",
    "    l = self.cnn_1(inputs['input'])\n",
    "    l = self.bn_1(l)\n",
    "    \n",
    "    l = self.cnn_2(l)\n",
    "    l = self.bn_2(l)\n",
    "\n",
    "    l = self.cnn_3(l)\n",
    "    l = self.bn_3(l)\n",
    "\n",
    "    l = self.cnn_4(l)\n",
    "    l = self.bn_4(l)\n",
    "\n",
    "    l = self.cnn_5(l)\n",
    "    l = self.bn_5(l)\n",
    "\n",
    "    l = self.drop_1(l)\n",
    "\n",
    "    l = self.flat(l)\n",
    "\n",
    "    l = self.dense_1(l)\n",
    "    l = self.bn_6(l)\n",
    "    l = self.drop_2(l)\n",
    "\n",
    "    l = self.dense_2(l)\n",
    "    l = self.bn_7(l)\n",
    "    l = self.drop_3(l)\n",
    "\n",
    "    l = self.dense_3(l)\n",
    "    l = self.bn_8(l)\n",
    "    l = self.drop_4(l)\n",
    "\n",
    "    l = self.dense_4(l)\n",
    "    l = self.bn_9(l)\n",
    "    \n",
    "    return {'angle' : self.output_layer_1(l), 'throttle' : self.output_layer_2(l)}\n",
    "\n",
    "model = DCModel(name='DonkeyCarModel')\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LR)\n",
    "model.compile(optimizer=optimizer,loss=keras.losses.MSE, metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv88oQBNCgb8"
   },
   "source": [
    "## 3. L'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Avx_bVFhRnHR"
   },
   "outputs": [],
   "source": [
    "#@title Les hyperparamètres de l'entrainement\n",
    "INITIAL_EPOQUE = 0\n",
    "NBR_EPOQUES = 1\n",
    "NBR_EPOQUES_APRES_EARLY_STOPPING = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IZfqjS7rRoSW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 151s 236ms/step - loss: 0.0443 - angle_loss: 0.0364 - throttle_loss: 0.0080 - angle_mse: 0.0364 - throttle_mse: 0.0080 - val_loss: 0.0128 - val_angle_loss: 0.0113 - val_throttle_loss: 0.0015 - val_angle_mse: 0.0113 - val_throttle_mse: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc8c10eac90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Définir mes callback\n",
    "cb = [\n",
    "    keras.callbacks.EarlyStopping(patience=NBR_EPOQUES_APRES_EARLY_STOPPING,\n",
    "                                  restore_best_weights=True),\n",
    "    keras.callbacks.TensorBoard(log_dir=TENSORLOG_PATH)\n",
    "    ]\n",
    "\n",
    "# Fit mon modèle\n",
    "model.fit(train_dataset,\n",
    "          validation_data=test_dataset,\n",
    "          epochs=NBR_EPOQUES,\n",
    "          initial_epoch = INITIAL_EPOQUE,\n",
    "          callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(SAVE_PATH, save_format='tf')\n",
    "model.save_weights(os.path.join(SAVE_PATH, \"weights.data\"))\n",
    "MODEL_SAVER.save(os.path.join(SAVE_PATH, \"model.code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCModelV1.4-renault-1614446893.405174\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DonkeyCarModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Conv2D)               multiple                  312       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  48        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              multiple                  4816      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch multiple                  192       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  96100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch multiple                  200       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  1275      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch multiple                  100       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch multiple                  100       \n",
      "_________________________________________________________________\n",
      "angle (Dense)                multiple                  26        \n",
      "_________________________________________________________________\n",
      "throttle (Dense)             multiple                  26        \n",
      "=================================================================\n",
      "Total params: 137,375\n",
      "Trainable params: 136,695\n",
      "Non-trainable params: 680\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJJKVYT_qDb8",
    "outputId": "3c218c0b-11d2-4a4c-be6a-907f0bc70d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 5s 86ms/step - loss: 0.0723 - angle_loss: 0.0419 - throttle_loss: 0.0304 - angle_mse: 0.0419 - throttle_mse: 0.0304\n",
      "Loss Angle : 0.07233501225709915 Loss Throttle : 0.041900672018527985 Acc Angle : 0.030434345826506615 Acc Throttle : 0.041900672018527985\n"
     ]
    }
   ],
   "source": [
    "performances = model.evaluate(validation_dataset, batch_size=32)\n",
    "print(\"Loss Angle :\", performances[0],\"Loss Throttle :\", performances[1],\"Acc Angle :\", performances[2],\"Acc Throttle :\", performances[3])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DonkeyCarSimulator-V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
